{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing with Dask\n",
    "### by [Richard W. Evans](https://sites.google.com/site/rickecon/), April 2019\n",
    "The code in this Jupyter notebook was written using Python 3.6.\n",
    "\n",
    "Parallel processing and high performance computing have become essential tools with the increase in the size and complexity of datasets and with the advances in dimensionality and solution algorithms of mathematical models. Further, simulation methods and statistical learning training methods benefit from the ability to scale computations on distributed systems like Google Cloud Services (GCS), Amazon Web Services (AWS), and Microsoft Azure.\n",
    "\n",
    "The standard laptop has multiple processors and usually comes equipped with a graphics card with hundreds of graphics processing units (GPUs). Each student's laptop can be transformed into a distributed computing system or, in other words, a supercomputer. Further, University of Chicago students have access to the [Midway cluster](https://rcc.uchicago.edu/resources/high-performance-computing) hosted by the [Research Computing Center (RCC)](https://rcc.uchicago.edu/). And finally, University of Chicago students with very sophisticated and high profile projects can apply for allocations on the [Argonne National Laboratory](https://www.anl.gov/) supercomputer, which has the [21st and 24th most powerful](https://www.top500.org/list/2018/11/) clusters in the world as of November 2018.\n",
    "\n",
    "We will focus on [Dask](https://dask.org/), which is the primary parallel processing library for Python. High performance computing has traditionally been executed on supercomputers through low-level languages such as C++ and Fortran. But most major supercomputers now support Python. Furthermore, Python has becoming the primary supervising architecture language at many of the national laboratories because it is quicker to write (developer time is more expensive than computer time) and because Python can serve as a wrapper for other low-level languages.\n",
    "\n",
    "In addition to Python being a supported language on most supercomputers, it is also a language with sophisticated parallel processing ability on whatever system you use personally--laptop, desktop, or high performance server.\n",
    "\n",
    "A great resource for learning high performance computing with Dask is the [Dask tutorial](https://github.com/dask/dask-tutorial) on GitHub. Another great resource is the Open Source Economics Laboratory (OSE Lab) [HPC training](https://github.com/sischei/OSM2018) by [Simon Scheidegger](https://sites.google.com/site/simonscheidegger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reasons we need HPC\n",
    "\n",
    "* Large data must be stored across multiple hard drives\n",
    "* Large data I/O must be optimized\n",
    "* Many technical computations can be parallelized\n",
    "* Simulation methods can be parallelized\n",
    "* Sampling methods can be parallelized\n",
    "\n",
    "Previous thinking was that we could just wait for processors and memory to become powerful enough, compact enough, and efficient enough to accomodate our increasing data sizes and methods complexity. However, the steady-increases in processor efficiency began to plateau in the mid 2000s. The March 12, 2016 issue of *The Economist* had an article entitled \"After Moore's Law\" that provided the following graphic of the trailing off of individual processor power.\n",
    "\n",
    "![Processor Plateau Fig](images/ProcessorPlateauFig.png \"Processor Plateau Fig\")\n",
    "\n",
    "Maximum clock speed has slowed despite the steady (exponential) increase in transistors per chip. The reasons for this slowdown are the following.\n",
    "\n",
    "* Hitting heat limits, cannot dissipate processing heat fast enough\n",
    "* Current leakage\n",
    "* Power consumption too high\n",
    "* Memory requirements too high\n",
    "\n",
    "But note that the price per transistor has also plateaued. The solution to this problem has been more parallel architecture and distributed systems as well as more specific architecture tailored to specific problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serial versus Parallel\n",
    "Serial computation is a set of instructions for which the order of computational instructions is important. Serial instruction tasks cannot be parallelized. Examples of computational tasks that must be serial are recursions and sorting.\n",
    "\n",
    "![Serial Process](images/SerialProcess.png \"Serial Process\")\n",
    "\n",
    "A parallel process breaks a problem up into pieces, executes the instructions associated with each piece, then brings the answers back together. Examples of computational tasks that can be parallelized are simulation, bootstrapping, some for-loops, pieces of numerical methods.\n",
    "\n",
    "![Parallel Process](images/ParallelProcess.png \"Parallel Process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Low-level to high-level parallel programming paradigms\n",
    "[Message passing interface (MPI)](https://en.wikipedia.org/wiki/Message_Passing_Interface) and OpenMP (open multi-processing) are the two main paradigms for performing high performance computing and parallel operations. These platforms are built on C, C++, and Fortran code, and are difficult to learn and to program. Most modern computer languages have bindings for these two platforms that allow users to access the functionality of MPI and OpenMP from the native environment of their high-level language of choice (e.g., Python, R, Java).\n",
    "\n",
    "Python has a particularly powerful library for parallel data I/O and parallel computation called [Dask](https://dask.pydata.org/en/latest/). A large number of great webcast tutorials and Jupyter notebooks for using Dask are available on the web. A particularly valuable [webcast Dask tutorial](https://www.youtube.com/watch?v=mbfsog3e5DA) came from the SciPy 2017 meeting in Austin Texas. All of the Jupyter notebooks from that session are available in the [https://github.com/dask/dask-tutorial](https://github.com/dask/dask-tutorial) repository. I recommend this tutorial to all researchers interested in learning parallel computation using Python. The following is a summary of the video.\n",
    "\n",
    "* (0:00-21:48) James Crist: Using Dask delayed\n",
    "* (21:53-56:59) Skipper Seabold: Dask DataFrames\n",
    "  * Dask DataFrames do not handle multiple pandas indices. Just choose the one that is the most intensive, and then choose subroutines to use any secondary indices\n",
    "  * Don't distribute stuff that will fit in memory. Don't add more complexity when not needed\n",
    "* (57:00-1:34:03) Martin Durant: Dask arrays (NumPy)\n",
    "  * Divides arrays into chunks that vary across all dimensions\n",
    "  * Dask DataFrames divides Pandas DataFrames into separate similar DataFrame chunks.\n",
    "  * Uses hdf5 data\n",
    "  * You want each piece of memory to be multiple megabytes but less than a gigabyte\n",
    "* (1:34:05-1:49:30) Skipper Seabold: Bag (parallel lists for semi-structured data)\n",
    "  * Good place to clean large data, messy data, nested data, transactional data\n",
    "  * Use foldby instead of groupby with bags\n",
    "* (1:49:30-2:06:28) James Crist: Schedulers\n",
    "  * Multiprocessing scheduler has some flaws\n",
    "  * Distributed scheduler fixes this\n",
    "  * Dask functionality with GPU's is a little limited\n",
    "* (2:06:37-2:20:58) Martin Durant: Distributed efficiency and DataFrames\n",
    "  * On distributed systems, want to store separate pandas dataframes in memory on multiple machines.\n",
    "  * Use the .persist method\n",
    "* (2:21:04-2:30:33) James Crist: Advanced distributed features\n",
    "  * Has functions like gather for asyncronous computations\n",
    "* (2:31:00-2:37:14) James Crist: History and progress of Dask, final Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Dask delayed\n",
    "Use Dask tutorial notebook [01_dask.delayed.ipynb](https://github.com/dask/dask-tutorial/blob/master/01_dask.delayed.ipynb). You will need to follow the instructions in the [README.md](https://github.com/dask/dask-tutorial/blob/master/README.md) on the [Dask tutorial page](https://github.com/dask/dask-tutorial) in order to be able to access the corresponding datasets and execute the cell blocks. Note that the `python prep.py` step takes a few minutes because it is creating a bunch of large data sets.\n",
    "\n",
    "Some notes.\n",
    "\n",
    "* We use the `time.sleep()` function to make simple functions take more time. This is trivial way to introduce functions that have measurable (> 1 second) computation time.\n",
    "* The `dask.delayed` decorator stages operations that are to be parallelized. Any function `dask.delayed` touches becomes lazy and runs later. Any data touched by `dask.delayed` will make any instruction that calls the data be delayed or lazy and run later.\n",
    "* `dask.compute()` runs the delayed objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Dask DataFrames (Pandas)\n",
    "Use Dask tutorial notebook [07_dataframe.ipynb](https://github.com/dask/dask-tutorial/blob/master/07_dataframe.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Using Dask arrays (NumPy)\n",
    " Use Dask tutorial notebook [05_array.ipynb](https://github.com/dask/dask-tutorial/blob/master/05_array.ipynb). This tutorial uses the HDF5 data format. [HDF5](https://www.hdfgroup.org/solutions/hdf5/) stands for hierarchical data format, and is a popular and efficient format in which to store large *n*-dimensional data or *n*-dimensional arrays. HDF5 files have a nice interface with both native python (`h5py` library) and through `Dask`.\n",
    "\n",
    "For working with HDF5 files in `Python`, you can go to the [`h5py` users manual](http://docs.h5py.org/en/latest/) or [FAQ](http://docs.h5py.org/en/latest/faq.html) page. Or you can go to the nice O'Reilley book by Collette (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. More complicated parallelism\n",
    "You can imagine much more complicated graphs of parallel processes. `Dask` allows you to perform more customized computation of parallel graphs in which you control what processes get sent to which workers.\n",
    "\n",
    "Use `Dask` tutorial notebook [04_distributed.ipynb](http://localhost:8889/notebooks/dask-tutorial/04_distributed.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Example\n",
    "Lets do a serial example and a parallel example in which we draw 10 datasets of 1 million elements each from a normal distribution $N(\\mu, \\sigma)$. From each of these datasets, we will compute the mean, standard deviation, and 90th percentile. In serial that is done by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "import timeit\n",
    "\n",
    "Sims = 12\n",
    "ObsPerSim = 1000000\n",
    "mu_norm = 10\n",
    "std_norm = 1.8\n",
    "seed_vec = np.arange(1, Sims + 1)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "results_ser = np.zeros([Sims, 3])\n",
    "\n",
    "for sim in range(Sims):\n",
    "    data = sts.lognorm.rvs(s=std_norm, scale=np.exp(mu_norm),\n",
    "                           size=ObsPerSim, random_state=seed_vec[sim])\n",
    "    results_ser[sim, 0] = data.mean()\n",
    "    results_ser[sim, 1] = data.std()\n",
    "    results_ser[sim, 2] = sorted(data)[900000]\n",
    "    \n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print('Elapsed time=', elapsed_time, 'seconds')\n",
    "    \n",
    "print(results_ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parallel, we can do that with the following code. We write a function that takes that parameters necessary for each of the 10 simulations, we start up the distributed scheduluer, we define the function as a `Dask.delayed` object, then we compute it using `Dask.compute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's check how many cores we have available\n",
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print('Number of available cores is', num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import compute, delayed\n",
    "import dask.multiprocessing\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "def sim_stats(sim_val, std, mu, obs, seed):\n",
    "    print('Working on simulation', sim_val)\n",
    "    data = sts.lognorm.rvs(s=std, scale=np.exp(mu), size=obs,\n",
    "                           random_state=seed)\n",
    "    results_mean = data.mean()\n",
    "    results_std = data.std()\n",
    "    results_90pct = sorted(data)[int(0.9 * obs)]\n",
    "    results = np.array([results_mean, results_std, results_90pct])\n",
    "    \n",
    "    return results\n",
    "\n",
    "lazy_values = []\n",
    "for sim in range(Sims):\n",
    "    lazy_values.append(delayed(sim_stats)(sim, std_norm, mu_norm, ObsPerSim, seed_vec[sim]))\n",
    "\n",
    "results_par = compute(*lazy_values, scheduler=dask.multiprocessing.get, num_workers=num_cores)\n",
    "\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print('Elapsed time=', elapsed_time, 'seconds')\n",
    "\n",
    "print(results_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "* Collette, Andrew, *Python and HDF5*, O'Reilly (2013).\n",
    "* *The Economist*, \"[After Moore's Law](https://www.economist.com/technology-quarterly/2016-03-12/after-moores-law)\", *The Economist*, (March 12, 2016)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
